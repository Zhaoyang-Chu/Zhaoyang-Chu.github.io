{"container_type": "Author", "filled": ["basics", "publications", "indices", "counts"], "scholar_id": "HYu3DyEAAAAJ", "source": "AUTHOR_PROFILE_PAGE", "name": "Zhaoyang Chu", "url_picture": "https://scholar.googleusercontent.com/citations?view_op=view_photo&user=HYu3DyEAAAAJ&citpid=2", "affiliation": "University College London (UCL)", "organization": 7707954445345430443, "interests": ["Artificial Intelligence", "Software Engineering", "Multimodal"], "email_domain": "@ucl.ac.uk", "homepage": "https://zhaoyang-chu.github.io/", "citedby": 178, "publications": {"HYu3DyEAAAAJ:u5HHmVD_uO8C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Hierarchical graph representation learning for the prediction of drug-target binding affinity", "pub_year": "2022"}, "filled": false, "author_pub_id": "HYu3DyEAAAAJ:u5HHmVD_uO8C", "num_citations": 66, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=7450322345783557213", "cites_id": ["7450322345783557213"]}, "HYu3DyEAAAAJ:d1gkVwhDpl0C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Testeval: Benchmarking large language models for test case generation", "pub_year": "2024"}, "filled": false, "author_pub_id": "HYu3DyEAAAAJ:d1gkVwhDpl0C", "num_citations": 46, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=8540604438409883947", "cites_id": ["8540604438409883947"]}, "HYu3DyEAAAAJ:u-x6o8ySG0sC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Graph Neural Networks for Vulnerability Detection: A Counterfactual Explanation", "pub_year": "2024"}, "filled": false, "author_pub_id": "HYu3DyEAAAAJ:u-x6o8ySG0sC", "num_citations": 34, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=15109084042234324415", "cites_id": ["15109084042234324415"]}, "HYu3DyEAAAAJ:IjCSPb-OGe4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Wait, We Don't Need to\" Wait\"! Removing Thinking Tokens Improves Reasoning Efficiency", "pub_year": "2025"}, "filled": false, "author_pub_id": "HYu3DyEAAAAJ:IjCSPb-OGe4C", "num_citations": 21, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=18012057152769628116", "cites_id": ["18012057152769628116"]}, "HYu3DyEAAAAJ:9yKSN-GCB0IC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Can Large Language Models Serve as Evaluators for Code Summarization?", "pub_year": "2024"}, "filled": false, "author_pub_id": "HYu3DyEAAAAJ:9yKSN-GCB0IC", "num_citations": 9, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=7299056823806911406", "cites_id": ["7299056823806911406"]}, "HYu3DyEAAAAJ:qjMakFHDy7sC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale", "pub_year": "2025"}, "filled": false, "author_pub_id": "HYu3DyEAAAAJ:qjMakFHDy7sC", "num_citations": 1, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=16393705663736702839", "cites_id": ["16393705663736702839"]}, "HYu3DyEAAAAJ:2osOgNQ5qMEC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "How to Select Pre-Trained Code Models for Reuse? A Learning Perspective", "pub_year": "2025"}, "filled": false, "author_pub_id": "HYu3DyEAAAAJ:2osOgNQ5qMEC", "num_citations": 1, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=2335344020783369353", "cites_id": ["2335344020783369353"]}, "HYu3DyEAAAAJ:zYLM7Y9cAGgC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning", "pub_year": "2025"}, "filled": false, "author_pub_id": "HYu3DyEAAAAJ:zYLM7Y9cAGgC", "num_citations": 0}}, "citedby5y": 178, "hindex": 5, "hindex5y": 5, "i10index": 4, "i10index5y": 4, "cites_per_year": {"2022": 2, "2023": 15, "2024": 38, "2025": 122}, "updated": "2025-10-26 08:15:07.017361"}